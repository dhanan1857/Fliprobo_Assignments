1 A
2 C
3 A
4 C
5 A
6 C
7 C
8 A
10. Reduction in overfitting: by averaging several trees, there is a significantly lower risk of overfitting.
    Less variance: By using multiple trees, you reduce the chance of stumbling across a classifier that doesnâ€™t 
   perform well because of the relationship between the train and test data.
11. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.
    Normalization and Standardization are two techniques.
12. Gradient descent which is an optimization algorithm often used in Logistic Regression, SVM, Neural Networks etc. 
    is another prominent example where if features are on different scale, certain weights are updated faster than others. 
    However, feature scaling helps in causing Gradient Descent to converge much faster as standardizing all the variables on to the same scale, 
    for example, for a linear regression makes it easy to calculate the slope ( y = mx + c) (where we normalize the M parameter to converge faster).
13.

14. F-score or F-measure) is a measure of a test's accuracy. It is calculated from the precision and recall of the test, 
    where the precision is the number of correctly identified positive results divided by the number of all positive results, 
    including those not identified correctly, and the recall is the number of correctly identified positive results divided by 
    the number of all samples that should have been identified as positive.
15. The fit() function calculates the values of these parameters. The transform function applies the values of the parameters on the actual data 
    and gives the normalized value. The fit_transform() function performs both in the same step. Note that the same value is got whether we perform in 
    2 steps or in a single step.